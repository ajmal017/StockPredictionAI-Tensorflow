{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StockPricePrediction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aOhGr00CE5bT"
      ],
      "authorship_tag": "ABX9TyPZ8xVFsdXrzKkMwxQxU2uD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tobiagru/StockPredictionAI-Tensorflow/blob/master/StockPricePrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaD5hMPyvv1H",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NDEhxosA2z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Deh6hlfnaDIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrOoUAJlUfFP",
        "colab_type": "text"
      },
      "source": [
        "## TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brXHzKJETuna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4BsHBRMUAe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  print(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91nYWzv_cVQb",
        "colab_type": "text"
      },
      "source": [
        "# Ideas\n",
        "\n",
        "based on https://towardsdatascience.com/aifortrading-2edd6fac689d,:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-x1U6ekdnmt",
        "colab_type": "text"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBSRyMzZ9k0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "from datetime import datetime, date\n",
        "import pickle as pkl\n",
        "from time import sleep\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import uuid\n",
        "import ipywidgets as widgets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKy7CD0ffW3t",
        "colab_type": "text"
      },
      "source": [
        "# BASE PATHS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vsJq5-BgB9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DRIVE_BASE = '/content/drive/My Drive/Project/StockPredictionAI'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxKUwRklEUEZ",
        "colab_type": "text"
      },
      "source": [
        "# Data / Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15dgU9f6U3IO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.DataFrame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZlfhf7oEeC3",
        "colab_type": "text"
      },
      "source": [
        "## News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOhGr00CE5bT",
        "colab_type": "text"
      },
      "source": [
        "### Crawl\n",
        "\n",
        "switch to \"https://wireapi.reuters.com/v8/feed/rcom/us/marketnews/ric:MSFT.OQ?until={last wireitem_id}\" \n",
        "- load new set of wireitems for last wireitem_id\n",
        "- extract url, wireitem_id\n",
        "- get articles for all urls and save them\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxRRCalRE6r0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install scrapy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39zPT2d1EVz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's get us some MSFT news, we are going to get the data from reuters\n",
        "import scrapy\n",
        "import logging\n",
        "import pickle as pkl\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "\n",
        "import os\n",
        "import psutil\n",
        "\n",
        "class ReutersSpider(scrapy.Spider):\n",
        "    name = 'blogspider'\n",
        "    #start_urls = ['https://www.reuters.com/article/us-amazon-com-pentagon/u-s-judge-says-amazon-likely-to-succeed-on-key-argument-in-contract-challenge-idUSKBN20U0QC']\n",
        "    #'https://www.reuters.com/search/news?blob=microsoft&sortBy=date&dateRange=all'\n",
        "    \n",
        "    custom_settings = {\n",
        "        'LOG_LEVEL': logging.WARNING,\n",
        "        'CONCURRENT_REQUESTS': 2,\n",
        "        'COOKIES_ENABLED': False\n",
        "    }\n",
        "    \n",
        "    def start_requests(self):\n",
        "        #load indexed_pages.pkl or assign set\n",
        "        urls = [\"https://www.reuters.com/assets/searchArticleLoadMoreJson?blob=microsoft&bigOrSmall=big&articleWithBlog=true&sortBy=date&dateRange=all&numResultsToShow=500&pn={}&callback=addMoreNewsResults\".format(idp) for idp in range(1,20)]\n",
        "        \n",
        "        for idx, url in enumerate(urls):\n",
        "            yield scrapy.Request(url, self.parse_search)\n",
        "    \n",
        "    def parse_search(self, response):\n",
        "        #the response is a js file #WhoDoesThat\n",
        "        text = response.text\n",
        "        start = text.find(\"news: [\") + 6\n",
        "        end = text.rfind(\"]\") + 1\n",
        "        content = text[start:end].replace('id:','\"id\":').\\\n",
        "                                  replace('headline:','\"headline\":').\\\n",
        "                                  replace('date:','\"date\":').\\\n",
        "                                  replace('href:','\"href\":').\\\n",
        "                                  replace('blurb:','\"blurb\":').\\\n",
        "                                  replace('mainPicUrl:','\"mainPicUrl\":')\n",
        "        pages = json.loads(content)\n",
        "\n",
        "        for page in pages:\n",
        "            if os.path.exists(\"reuters/{}.json\".format(page[\"date\"])):\n",
        "                print(\"Skipping \", page[\"date\"], \" already exists\")\n",
        "                continue\n",
        "                    \n",
        "            time.sleep(random.randrange(3,13))\n",
        "            \n",
        "            process = psutil.Process(os.getpid())\n",
        "            print(\"{:.2f} mb - Working on {}\".format(process.memory_info().rss / 1000000, page[\"date\"]))\n",
        "            yield scrapy.Request(\"https://www.reuters.com/{}\".format(page[\"href\"]),\n",
        "                                 callback=self.parse,\n",
        "                                 priority=99,\n",
        "                                 cb_kwargs={\"date\":page[\"date\"]})\n",
        "                \n",
        "    def parse(self, response, date):        \n",
        "        content = [response.css('h1.ArticleHeader_headline::text').get()]\n",
        "        content += response.css('div.StandardArticleBody_body').xpath('//p/text()').getall()\n",
        "        \n",
        "        print(date, \" ... save\")\n",
        "        \n",
        "        with open('reuters/{}.json'.format(date), 'w+') as file:\n",
        "            file.write(json.dumps(content))\n",
        "        \n",
        "# we need an itemPipeline that get's rid of "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJWbkgEKEbSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "process = CrawlerProcess({\n",
        "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
        "})\n",
        "\n",
        "process.crawl(ReutersSpider)\n",
        "process.start()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4eBHD3BsdZz",
        "colab_type": "text"
      },
      "source": [
        "### Sentiment Analysis\n",
        "\n",
        "Dimensions of Sentiment Analysis:  \n",
        "Sensitivity = Text is [Postive (1), Neutral (0), Negative (-1)]  \n",
        "Specificity = Text specific to MSFT: [High(1), Medium(0), Low(-1)]  \n",
        "Objectiveness = Text is written [Objective(1), Mixed(0), Subjective(-1)]  \n",
        "Intensity = Text is written [Emotionally (1), Mixed (0), Distant (-1)]  \n",
        "\n",
        "---\n",
        "\n",
        "Based on BERT paper: https://arxiv.org/abs/1810.04805  \n",
        "\n",
        "using BERT model from tf.hub.  \n",
        "* https://tfhub.dev/google/collections/bert/1\n",
        "* https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb#scrollTo=y-wDZJCCTgtC \n",
        "* https://github.com/tensorflow/models/official/nlp/bert \n",
        "\n",
        "---\n",
        "\n",
        "The stuff runs on TPU so we are using that to fine tune\n",
        "\n",
        "---\n",
        "\n",
        "Finetuning Dataset:\n",
        "* ???\n",
        "* Get random news (not for MSFT) from reuters for related companies. Take the stop movement the days after the news compared to before no change in trend = neutral, upward change = positive, downward = negative (This might lead to bias as we are trying to predict movement and the dataset tells the movement) - maybe AAPL, GOOGL, ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yaXScapdvzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!test -d models || git clone https://github.com/tensorflow/models.git models\n",
        "if not 'models' in sys.path:\n",
        "  sys.path += ['models']\n",
        "!export PYTHONPATH=\"$PYTHONPATH:/content/models/official/nlp/bert\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RK7XYKqdERK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "from tokenization import FullTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOYmeHXsM5M4",
        "colab_type": "text"
      },
      "source": [
        "### Import & Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3F0RraWeXzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BERT_MODEL_HUB = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1'\n",
        "\n",
        "BUCKET_BASE_DIR = 'gs://.../bert/'\n",
        "#OUTPUT_DIR = BUCKET_BASE_DIR + 'models'\n",
        "\n",
        "TEST_RATIO = 10 #%\n",
        "EVAL_RATIO = 20 #%\n",
        "\n",
        "NUM_OUTPUTS = 4\n",
        "\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 8\n",
        "PREDICT_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "MAX_SEQ_LENGTH = 128\n",
        "WARMUP_PROPORTION = 0.1\n",
        "\n",
        "#Checkpoints\n",
        "CHECKPOINTS_DIR = BUCKET_BASE_DIR + 'checkpoints'\n",
        "#check dir exists else create\n",
        "CHECKPOINTS_NAME = os.path.join(CHECKPOINTS_DIR, 'bert_sensitivity.ckpt')\n",
        "CHECKPOINTS_STEPS = 1000\n",
        "\n",
        "#Summary\n",
        "SUMMARY_DIR = BUCKET_BASE_DIR + 'summaries'\n",
        "#check dir exists else create\n",
        "SUMMARY_STEPS = 500\n",
        "\n",
        "#Model Save\n",
        "MODEL_SAVE_PATH = BUCKET_BASE_DIR + 'models'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRd1pXQ_Ibqb",
        "colab_type": "text"
      },
      "source": [
        "### Prepare Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-nsJECJOxqX",
        "colab_type": "text"
      },
      "source": [
        "Further Data:\n",
        "* Financial Sentiment Lexicon: https://sraf.nd.edu/textual-analysis/resources/#LM%20Sentiment%20Word%20Lists\n",
        "* Go for the Thomson Reuters News Archive and use multi language & multi source coverage together with a multi language BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKoqouh7NpcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert Finetuning Set\n",
        "# --> not \"free\" suitable found, have to label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqHHPEWgcT_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# THIS ONLY RUNS IN NORMAL JUPYTER NOTEBOOK NOT IN COLAB\n",
        "\n",
        "%gui asyncio\n",
        "import asyncio\n",
        "\n",
        "# WIP : Helper to label the reuters news\n",
        "\n",
        "labeled_ratio = 10 #%\n",
        "\n",
        "msft_keys = ['microsoft', 'msft', 'windows', 'office365', 'azure', 'xbox', 'surface']\n",
        "\n",
        "known_segments = set()\n",
        "paragraph_list = [['file','assignment']]\n",
        "\n",
        "def wait_for_change(widget):\n",
        "    future = asyncio.Future()\n",
        "    def getvalue(change):\n",
        "        future.set_result(change.description)\n",
        "        widget.on_click(getvalue, remove=True)  \n",
        "    widget.on_click(getvalue)\n",
        "    return future\n",
        "\n",
        "sensitivity = widgets.ToggleButtons(\n",
        "    options=['Positive', 'Neutral', 'Negative'],\n",
        "    value='Neutral',\n",
        "    description='Sensitivity:',\n",
        "    disabled=False,\n",
        "    button_style='',\n",
        ")\n",
        "\n",
        "specificity = widgets.ToggleButtons(\n",
        "    options=['High', 'Medium', 'Low'],\n",
        "    value='Medium',\n",
        "    description='Specificitiy:',\n",
        "    disabled=False,\n",
        "    button_style='',\n",
        ")\n",
        "\n",
        "objectiveness = widgets.ToggleButtons(\n",
        "    options=['Objective', 'Mixed', 'Subjective'],\n",
        "    value='Mixed',\n",
        "    description='Objectiveness:',\n",
        "    disabled=False,\n",
        "    button_style='',\n",
        ")\n",
        "\n",
        "intensity = widgets.ToggleButtons(\n",
        "    options=['Emotional', 'Mixed', 'Distant'],\n",
        "    value='Mixed',\n",
        "    description='Intensity:',\n",
        "    disabled=False,\n",
        "    button_style='',\n",
        ")\n",
        "\n",
        "text = widgets.Text(\n",
        "    value='...',\n",
        "    description='String:',\n",
        "    disabled=True\n",
        ")\n",
        "\n",
        "button = widgets.Button(description='Next')\n",
        "\n",
        "async def f():\n",
        "    for path, subdir, articles in tf.io.gfile.walk('reuters'):\n",
        "        for article in articles:\n",
        "            with open(os.path.join(path, article)) as file:\n",
        "                segments = json.loads(file.read())\n",
        "\n",
        "            for segment in segments:\n",
        "                segment_uuid = uuid.uuid1()\n",
        "                segment = segment.lower()\n",
        "\n",
        "                #dump segments that don't include any microsoft related key\n",
        "            if not any([key in segment for key in msft_keys]):\n",
        "                continue\n",
        "\n",
        "            #dump segments that are duplicates\n",
        "            #TODO depending on how many segments you have and what kind if VM you might be better of with a btree based on words or dumping it to file\n",
        "            if segment in known_segments:\n",
        "                continue\n",
        "            else:\n",
        "                known_segments.add(segment)\n",
        "\n",
        "            #show random segments for labeling\n",
        "            if random.randint(0,99) < labeled_ratio:\n",
        "                text.value = segment\n",
        "                segment_json = {'text': segment,\n",
        "                                'sensitivity': sensitivity.value,\n",
        "                                'specificity': specificity.value,\n",
        "                                'objectiveness': objectiveness.value,\n",
        "                                'intensity': intensity.value}\n",
        "\n",
        "                await wait_for_change(button)\n",
        "                print(segment_json)\n",
        "                randint = random.randint(0,99)\n",
        "                if randint < TEST_RATIO:\n",
        "                    assigment = 'test'\n",
        "                elif randint < TEST_RATIO + EVAL_RATIO:\n",
        "                    assigment = 'eval'\n",
        "                else:\n",
        "                    assigment = 'train'\n",
        "                print()\n",
        "            else:\n",
        "                segment_json = {'text': segment}\n",
        "                assigment = 'unlabeled'\n",
        "\n",
        "            fname = os.path.join(article.split(\".\")[0], \"{}.txt\".format(segment_uuid))\n",
        "            paragraph_list.append((fname, assigment))\n",
        "            print((fname, assigment))\n",
        "            \n",
        "            tf.io.gfile.makedirs(os.path.join(os.getcwd(), 'msft_segments', article.split(\".\")[0]))\n",
        "            with open(os.path.join(os.getcwd(), 'msft_segments', fname), 'w+') as file:\n",
        "                file.write(json.dumps(segment_json))\n",
        "\n",
        "    with open(os.path.join(os.getcwd(), 'msft_segments', 'file_list.csv'), \"w+\") as file:\n",
        "        for line in paragraph_list:\n",
        "            file.write(\",\".join(line))\n",
        "            file.write(\"\\n\")\n",
        "\n",
        "\n",
        "asyncio.ensure_future(f())\n",
        "display(text,\n",
        "        sensitivity,\n",
        "        specificity,\n",
        "        objectiveness,\n",
        "        intensity,\n",
        "        button)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC8y9YjIgi36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from https://medium.com/@brn.pistone/bert-fine-tuning-for-tensorflow-2-0-with-keras-api-9913fc1348f6\n",
        "def get_masks(tokens):\n",
        "  \"\"\"Mask for padding\"\"\"\n",
        "  if len(tokens)>MAX_SEQ_LENGTH:\n",
        "      raise IndexError(\"Token length more than max seq length!\")\n",
        "  return [1]*len(tokens) + [0] * (MAX_SEQ_LENGTH - len(tokens))\n",
        "\n",
        "\n",
        "def get_segments(tokens):\n",
        "  \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "  if len(tokens)>MAX_SEQ_LENGTH:\n",
        "      raise IndexError(\"Token length more than max seq length!\")\n",
        "  segments = []\n",
        "  current_segment_id = 0\n",
        "  for token in tokens:\n",
        "      segments.append(current_segment_id)\n",
        "      if token == \"[SEP]\":\n",
        "          current_segment_id = 1\n",
        "  return segments + [0] * (MAX_SEQ_LENGTH - len(tokens))\n",
        "\n",
        "\n",
        "def get_ids(tokens, tokenizer):\n",
        "  \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "  token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  input_ids = token_ids + [0] * (MAX_SEQ_LENGTH-len(token_ids))\n",
        "  return input_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul8OK7_AM-Gr",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baAZYim1JcaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#############\n",
        "## Dataset Generator\n",
        "#############\n",
        "\n",
        "#batch_size=TRAIN_BATCH_SIZE,\n",
        "\n",
        "def get_dataset_generator(dir, bert_layer, batch_size, train=True):\n",
        "  def dataset():\n",
        "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "\n",
        "    batch = []\n",
        "\n",
        "    for path, subdir, paragraph_file in tf.io.gfile.walk(os.path.join(DRIVE_BASE,dir)):\n",
        "      with open(os.path.join(path, subdir, paragraph_file)) as file:\n",
        "        paragraph = json.loads(file.read())\n",
        "\n",
        "      tokens = tokenizer.tokenize(paragraph)\n",
        "      token_ids = get_ids(tokens, tokenizer)\n",
        "      mask = get_masks(tokens)\n",
        "      segment_ids = get_segments(tokens)\n",
        "      \n",
        "      batch.append((token_ids, mask, segment_ids), )\n",
        "\n",
        "      if len(batch) == batch_size:\n",
        "        yield batch\n",
        "        batch = []\n",
        "  \n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H605u13jfrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########\n",
        "## Model\n",
        "###########\n",
        "\n",
        "def create_model(bert_layer):\n",
        "  \"\"\"\n",
        "  takes the bert_layer and creates a finetuning ready NN from it\n",
        "  by adding classification (Fully + Softmax) at the end\n",
        "  \"\"\"\n",
        "  # Inputs\n",
        "  input_word_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype=tf.int32,name=\"input_word_ids\")\n",
        "  input_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype=tf.int32,name=\"input_mask\")\n",
        "  segment_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype=tf.int32,name=\"segment_ids\")\n",
        "  \n",
        "  # Embedding (BERT)\n",
        "  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "  \n",
        "  # Classification\n",
        "  x = tf.keras.layers.Flatten()(pooled_output) #we only need the [CLS] token\n",
        "  x = tf.keras.layers.Dense(256, activation=tf.keras.activations.relu)(x)\n",
        "  output = tf.keras.layers.Dense(NUM_OUTPUTS, activation=tf.keras.activations.linear)(x)\n",
        "  \n",
        "  # Model\n",
        "  model = tf.keras.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=output)\n",
        "\n",
        "  # TPU Strategy\n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "  # Compile\n",
        "  with strategy.scope():\n",
        "    model = create_model(bert_layer)\n",
        "    model.compile(\n",
        "        optimizer='adam', #tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), <-- this fails\n",
        "        loss=tf.keras.losses.mean_squared_error,\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-oA6KDjFIpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEdPgPPD70ZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################\n",
        "## Finetuning\n",
        "#################\n",
        "\n",
        "# Callbacks\n",
        "checkpoint_callback  = tf.keras.callbacks.ModelCheckpoint(filepath=CHECKPOINTS_NAME,\n",
        "                                                 save_weights_only=True)\n",
        "summary_callback = tf.keras.callbacks.TensorBoard(SUMMARY_DIR)\n",
        "# Build Model\n",
        "\n",
        "model = create_model(bert_layer)\n",
        "\n",
        "# Train & Evaluate\n",
        "model.fit(\n",
        "    x=get_dataset_generator(dir, bert_layer),\n",
        "    epochs=NUM_TRAIN_EPOCHS,\n",
        "    validation_data=(testing_set, testing_label),\n",
        "    verbose=1,\n",
        "    callbacks=[checkpoint_callback, summary_callback],\n",
        "    use_multiprocessing=True\n",
        ")\n",
        "\n",
        "#model save\n",
        "model.save_weights(MODEL_SAVE_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX13hqGjsZ9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict Sentiment for all sentences and aggregate (sum) for article\n",
        "if not model:\n",
        "  bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=True)\n",
        "  model = create_model(bert_layer)\n",
        "  model.load_weights(MODEL_SAVE_PATH)\n",
        "\n",
        "batch = []\n",
        "tmp_result = np.zeros((PREDICT_BATCH_SIZE,5))\n",
        "results = pd.DataFrame\n",
        "\n",
        "for path, subdir, paragraph_file in tf.io.gfile.walk(os.path.join(DRIVE_BASE,dir)):\n",
        "  with open(os.path.join(path, subdir, paragraph_file)) as file:\n",
        "    paragraph = json.loads(file.read())\n",
        "\n",
        "  tokens = tokenizer.tokenize(paragraph)\n",
        "  token_ids = get_ids(tokens, tokenizer)\n",
        "  mask = get_masks(tokens)\n",
        "  segment_ids = get_segments(tokens)\n",
        "\n",
        "  tmp_result[len(batch),0] = subdir\n",
        "  batch.append((token_ids, mask, segment_ids))\n",
        "  \n",
        "  if len(batch) == PREDICT_BATCH_SIZE:\n",
        "    tmp_result[:,1:] = model.predict(batch)\n",
        "    cols = [\"timestamp\",\"News_Sensitivity\",\"News_Specificity\",\"News_Objectiveness\",\"News_Intensity\"]\n",
        "    result[cols] = tmp_result\n",
        "    batch = []\n",
        "    tmp_result = np.zeros((PREDICT_BATCH_SIZE,5))\n",
        "\n",
        "results['timestamp'] = pd.to_datetime(results['timestamp'], format=\"%B %d,%Y %I-%M%p %Z\")\n",
        "results = results.resample('D', on='timestamp').average().set_index('timestamp')\n",
        "data = data.merge(results, how='outer', left_index=True, right_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKra5CwHEhbU",
        "colab_type": "text"
      },
      "source": [
        "## Stocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XveZCIjE14b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install alpha_vantage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaTaBvA6FS0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import os\n",
        "#key = 'your key'\n",
        "#with open(os.path.join(DRIVE_BASE,'alpha_vantage_key.txt'), 'w+') as file:\n",
        "#  file.write(key)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBhVLiEBPL49",
        "colab_type": "text"
      },
      "source": [
        "**Stocks to use:**\n",
        "Major Business MSFT:\n",
        "- Windows & Office\n",
        "- Cloud\n",
        "- Professional Software (ERP, Server, DB)\n",
        "- Gaming\n",
        "Impacts on MSFT:\n",
        "- Global Trade (FX, {Import/Export})\n",
        "- Healthyness of Companies (Stock Index, {GDP}, ...)\n",
        "- {Consumer Buying Power - (CPI)}\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldKE5UF2PLMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "msft_symbol = 'MSFT'\n",
        "\n",
        "correlated_assets_symbols = [\n",
        "    'AAPL',\n",
        "    'GOOGL',\n",
        "    'AMZN',\n",
        "    'CRM', # Salesforce\n",
        "    'SAP',\n",
        "    '6758.TOK', # Sony\n",
        "    'BABA', #Alibaba\n",
        "    'IBM',\n",
        "    'DELL',\n",
        "    'HPQ', #HP\n",
        "    'ORCL', # Oracle\n",
        "    #'IXIC', #NasDaq -- doesn't work\n",
        "    'DJI', # Dow Jones\n",
        "    #'^NI225', # Nikkei\n",
        "    'SPX', # S&P 500\n",
        "    #'UKX', # FTSE\n",
        "    'DAX',\n",
        "    'HSI', # Heng Seng\n",
        "    'ACN', # Accenture\n",
        "    'INFY', # Infosys\n",
        "    'DXC',\n",
        "    'CTSH', # Cognizant\n",
        "    'HCLTECH.NSE', #HCL\n",
        "    'TCS', # Tata Consulting Services\n",
        "    'WIPRO.NSE',\n",
        "    'ATVI', #Activision\n",
        "    'TCEHY', #Tencent\n",
        "    'TTWO', # Take-Two Interactive\n",
        "    'EA',\n",
        "    'HUYA',\n",
        "]\n",
        "\n",
        "fx_symbols = [\n",
        "    ('USD','JPY'),\n",
        "    ('USD','GBP'),\n",
        "    ('USD','EUR'),\n",
        "    ('USD','CNY'),\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YIeEMAVdvRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "def alpha_vantage_request(function,\n",
        "                          apikey_file=os.path.join(DRIVE_BASE,'alpha_vantage_key.txt'),\n",
        "                          **kwargs):\n",
        "  \"\"\"\n",
        "  expected kwargs:\n",
        "   * TIME_SERIES_DAILY: symbol=<symbol>, outputsize='full'\n",
        "   * FX_DAILY: from_symbol=<currency>, to_symbol=<currency>, outputsize='full'\n",
        "  \"\"\"\n",
        "  \n",
        "  with open(apikey_file, 'r') as file:\n",
        "    apikey = file.read()\n",
        "  \n",
        "  url = \"https://www.alphavantage.co/query\"\n",
        "\n",
        "  params = {'function':function,\n",
        "            'apikey':apikey}\n",
        "\n",
        "  params.update(kwargs)\n",
        "\n",
        "  r = requests.get(url=url, params=params)\n",
        "\n",
        "  if r.status_code == 200:\n",
        "    return r.json() \n",
        "  else:\n",
        "    raise KeyError(\"{} - {}\".format(r.status_code, r.text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF8fqrCm4Ju2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if os.path.exists(os.path.join(DRIVE_BASE, 'msft.pkl')):\n",
        "  print('MSFT ... loading from file')\n",
        "  with open(os.path.join(DRIVE_BASE, 'msft.pkl'), 'rb') as file:\n",
        "    msft = pkl.load(file)\n",
        "else:\n",
        "  print('MSFT ... loading from API')\n",
        "  msft = alpha_vantage_request('TIME_SERIES_DAILY',\n",
        "                             symbol=msft_symbol,\n",
        "                             outputsize='full')\n",
        "  with open(os.path.join(DRIVE_BASE, 'msft.pkl'), 'wb+') as file:\n",
        "    pkl.dump(msft, file)\n",
        "  \n",
        "msft_df = pd.DataFrame(msft['Time Series (Daily)'], dtype=float).T\n",
        "msft_df.index = pd.to_datetime(msft_df.index)\n",
        "msft_df = msft_df.drop(['1. open', '2. high', '3. low', '5. volume'], axis=1)\n",
        "msft_df = msft_df.rename(columns={'4. close':'MSFT'})\n",
        "data = data.merge(msft_df, how='outer', left_index=True, right_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0kK5LGV9mjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(data.loc['2010', 'MSFT'], label='MSFT stock')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('USD')\n",
        "plt.title('MSFT stock price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ry2gKzjh5rn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save = False\n",
        "if os.path.exists(os.path.join(DRIVE_BASE, 'correlated_assets.pkl')):\n",
        "  print('Correlated Assets ... loading from file')\n",
        "  with open(os.path.join(DRIVE_BASE, 'correlated_assets.pkl'), 'rb') as file:\n",
        "    correlated_assets = pkl.load(file)\n",
        "else:\n",
        "  correlated_assets = {}\n",
        "  save = True\n",
        "\n",
        "N_idx = len(correlated_assets_symbols)\n",
        "for idx, symbol in enumerate(correlated_assets_symbols):\n",
        "  if not symbol in correlated_assets:\n",
        "    print(\"{}/{}\".format(idx,N_idx), symbol, ' ... loading')\n",
        "    correlated_assets[symbol] = alpha_vantage_request('TIME_SERIES_DAILY',\n",
        "                                                      symbol=symbol,\n",
        "                                                      outputsize='full')\n",
        "    save = True\n",
        "    sleep(15)\n",
        "  else:\n",
        "    print(\"{}/{}\".format(idx,N_idx), symbol, ' ... skipping, already exists')\n",
        "\n",
        "if save:\n",
        "  with open(os.path.join(DRIVE_BASE, 'correlated_assets.pkl'), 'wb+') as file:\n",
        "    pkl.dump(correlated_assets, file)\n",
        "\n",
        "idx = 0\n",
        "for symbol, asset in correlated_assets.items():\n",
        "  idx += 1\n",
        "  print(\"{}/{}\".format(idx,N_idx), symbol, ' ... merging')\n",
        "  tmp_df = pd.DataFrame(asset['Time Series (Daily)'], dtype=float).T\n",
        "  tmp_df.index = pd.to_datetime(tmp_df.index)\n",
        "  tmp_df = tmp_df.drop(['1. open', '2. high', '3. low', '5. volume'], axis=1)\n",
        "  tmp_df = tmp_df.rename(columns={'4. close':symbol})\n",
        "  data = data.merge(tmp_df, how='outer', left_index=True, right_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb368PgSJ4HR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save = False\n",
        "if os.path.exists(os.path.join(DRIVE_BASE, 'fx_rates.pkl')):\n",
        "  print('FX Rates ... loading from file')\n",
        "  with open(os.path.join(DRIVE_BASE, 'fx_rates.pkl'), 'rb') as file:\n",
        "    fx_rates = pkl.load(file)\n",
        "else:\n",
        "  fx_rates = {}\n",
        "  save = True\n",
        "\n",
        "N_idx = len(fx_symbols)\n",
        "for idx, fx in enumerate(fx_symbols):\n",
        "  if not fx[0] + fx[1] in fx_rates:\n",
        "    print(\"{}/{}\".format(idx,N_idx), fx[0] + fx[1], ' ... loading')\n",
        "    fx_rates[fx[0] + fx[1]] = alpha_vantage_request('FX_DAILY',\n",
        "                                                    from_symbol=fx[0],\n",
        "                                                    to_symbol=fx[1],\n",
        "                                                    outputsize='full')\n",
        "    sleep(15)\n",
        "  else:\n",
        "    print(\"{}/{}\".format(idx,N_idx), fx[0] + fx[1], ' ... skipping, already exists')\n",
        "\n",
        "if save:\n",
        "  with open(os.path.join(DRIVE_BASE, 'fx_rates.pkl'), 'wb+') as file:\n",
        "    pkl.dump(fx_rates, file)\n",
        "\n",
        "idx = 0\n",
        "for symbol, fx in fx_rates.items():\n",
        "  idx += 1\n",
        "  print(\"{}/{}\".format(idx,N_idx), symbol, ' ... merging')\n",
        "  tmp_df = pd.DataFrame(fx['Time Series FX (Daily)'], dtype=float).T\n",
        "  tmp_df.index = pd.to_datetime(tmp_df.index)\n",
        "  tmp_df = tmp_df.drop(['1. open', '2. high', '3. low'], axis=1)\n",
        "  tmp_df = tmp_df.rename(columns={'4. close':symbol})\n",
        "  data = data.merge(tmp_df, how='outer', left_index=True, right_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTyKDgoPEwJ5",
        "colab_type": "text"
      },
      "source": [
        "### Technical Indicators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Lz4ZlINqSJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Only for MSFT\n",
        "symbol='MSFT'\n",
        "\n",
        "save = False\n",
        "if os.path.exists(os.path.join(DRIVE_BASE, 'indicators.pkl')):\n",
        "  print('FX Rates ... loading from file')\n",
        "  with open(os.path.join(DRIVE_BASE, 'indicators.pkl'), 'rb') as file:\n",
        "    indicators = pkl.load(file)\n",
        "else:\n",
        "  indicators = {}\n",
        "  save = True\n",
        "\n",
        "indicator_kwargs = {\n",
        "    'SMA7': {'function':'SMA', 'symbol':symbol, 'interval':'daily', 'time_period':'7', 'series_type':'close'},\n",
        "    'SMA21': {'function':'SMA', 'symbol':symbol, 'interval':'daily', 'time_period':'21', 'series_type':'close'},\n",
        "    'EMA14': {'function':'EMA', 'symbol':symbol, 'interval':'daily', 'time_period':'14', 'series_type':'close'},\n",
        "    'EMA29': {'function':'EMA', 'symbol':symbol, 'interval':'daily', 'time_period':'29', 'series_type':'close'},\n",
        "    'BBANDS': {'function':'BBANDS', 'symbol':symbol, 'interval':'daily', 'time_period':'20', 'series_type':'close'},\n",
        "    'MACD': {'function':'MACD', 'symbol':symbol, 'interval':'daily', 'series_type':'close'},\n",
        "    'RSI': {'function':'RSI', 'symbol':symbol, 'interval':'daily', 'time_period':'14', 'series_type':'close'},\n",
        "    'ADX': {'function':'ADX', 'symbol':symbol, 'interval':'daily', 'time_period':'14'}\n",
        "}\n",
        "\n",
        "for key, kwarg in indicator_kwargs.items():\n",
        "  if not key in indicators:\n",
        "    print(key, ' ... loading')\n",
        "    indicators[key] = alpha_vantage_request(**kwarg)\n",
        "    save = True\n",
        "    sleep(15)\n",
        "  else:\n",
        "    print(key, ' ... skipping, already exists')\n",
        "\n",
        "if save:\n",
        "  with open(os.path.join(DRIVE_BASE, 'indicators.pkl'), 'wb+') as file:\n",
        "    pkl.dump(indicators, file)\n",
        "\n",
        "N_idx = len(indicators)\n",
        "idx = 0\n",
        "for symbol, fx in indicators.items():\n",
        "  idx += 1\n",
        "  print(\"{}/{}\".format(idx,N_idx), symbol, ' ... merging')\n",
        "  tmp_df = pd.DataFrame(fx[list(fx.keys())[1]], dtype=float).T\n",
        "  tmp_df.index = pd.to_datetime(tmp_df.index)\n",
        "  tmp_df = tmp_df.rename(columns={col:'{}_{}'.format(symbol,col) for col in tmp_df.keys()})\n",
        "  data = data.merge(tmp_df, how='outer', left_index=True, right_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeJlu5lkLuG4",
        "colab_type": "text"
      },
      "source": [
        "### Fourier Transform\n",
        "Used to denoise the stock movement\n",
        "\n",
        "**THE MSFT STOCK DOESNT OSCILLATE ENOUGH FOR FFT TO WORK***   \n",
        "**--> NOT INCLUDED IN FEATUERS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCkjV-bff7Cz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_components = [3,12,48,188] #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1L5GV9phmXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fft = np.fft.fft(data.loc['2010':'2019', 'MSFT'].dropna().to_numpy())\n",
        "a = np.fft.ifft(fft).astype(float)\n",
        "bs = {}\n",
        "for num_c in num_components:\n",
        "  tmp_fft = np.zeros(fft.shape, dtype=np.csingle)\n",
        "  tmp_fft[:num_c] = fft[:num_c]\n",
        "  bs['FFT-{}'.format(num_c)] = (np.fft.ifft(tmp_fft).astype(float))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY38S89PiQ8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(a, label='FFT-Full')\n",
        "for key, b in bs.items():\n",
        "  plt.plot(b, label=key)\n",
        "plt.xlabel('Days')\n",
        "plt.ylabel('USD')\n",
        "plt.title('MSFT stock price from FFT')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVv9eqvzYor-",
        "colab_type": "text"
      },
      "source": [
        "### Arima\n",
        "Not used to predict but to denoise the stock movement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQxMrITwYnZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow-4Vqk6aHie",
        "colab_type": "text"
      },
      "source": [
        "### High Level Features\n",
        "UNET to extract  \n",
        "ULMNet to reduce dimensionality  \n",
        "--> nore sure this makes sense but let's see"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFZeX03vaHH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K70qJj1dcpwN",
        "colab_type": "text"
      },
      "source": [
        "## Train/Test Split\n",
        "Split by date and make something like the last two year as test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMBovD-0c0zY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only include 2010 2019 - the 2020 chrisis might be a hard one\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq8L7W07c1pd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dump the test and train dataset and upload to GCS bucket\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNtY6NjBb3Ul",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "- let's start with just a LSTM, transformer and a CNN for prediciting the timeseries. We can always add the GAN later on\n",
        "- Maybe turn it into a multitask learning exercise where it has to predict multiple stocks at the same time.\n",
        "- Maybe use a transformer just on the stocks to predict trend, momentum, ... to see if the transformer is able to create new technical indicators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N9mQ7G610jn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B09GOgMm11gU",
        "colab_type": "text"
      },
      "source": [
        "## Feature Evaluation & Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuRaAeCTb1Xs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y86l4Dg2caBC",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiCO0KOwcbhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}